{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import requests\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "#connect to MongoDB using pymongo\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['13F']\n",
    "collection = db['13F_collection']\n",
    "\n",
    "def main():\n",
    "    #open the csv containing CIK number for reading\n",
    "    with open('/Users/a/Desktop/KSU/GRA/CSV/13f/13f test.csv' , newline= '') as csv_file:\n",
    "        cik_reader = csv.reader(csv_file)\n",
    "        diction_lists = []\n",
    "        #check_point will keep track of what iteration the script is on by printing 1.. 2.. 3.. etc..\n",
    "        check_point = 0\n",
    "        for x in cik_reader:\n",
    "            check_point = check_point + 1\n",
    "            print(check_point, x)\n",
    "            cik = ''.join(x)\n",
    "            url_func(cik)\n",
    "\n",
    "#Url where form 4 data will be retrieved\n",
    "def url_func(cik_num):\n",
    "    \n",
    "    #insert variable will be appended to the end of the URL. Insert variable integer will display number of links\n",
    "    insert = 0\n",
    "    for b in range(10):\n",
    "        \n",
    "        #this will return the page that links to where CIK documents are stored. The part of url that reads 'page=' will set the amount of form 4's. Max # is 100\n",
    "        url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=' + cik_num + '&type=13f&dateb=&owner=include&start=' + str(insert) + '&count=100'\n",
    "        insert = insert + 100\n",
    "        print(url)\n",
    "        \n",
    "        #This section spoofs a user agent to prevent blocking of scripts\n",
    "        time.sleep(.5)\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "        spoof = requests.get(url, headers=headers)\n",
    "        source = spoof.text\n",
    "        \n",
    "        #run beautifulSoup on returned data\n",
    "        source_bs = bs.BeautifulSoup(source)\n",
    "\n",
    "        #parse data to find all tags with 'a' (which are links) and an id of 'documentsbutton'\n",
    "        #these links will need to have \"https://www.sec.gov\" appended to the front of them in next step\n",
    "        document_id = source_bs.find_all(\"a\", id=\"documentsbutton\")\n",
    "        \n",
    "        #this loop will append \"https://www.sec.gov\" to the front of the links in 'document_id'\n",
    "        #link_list[] will then contain the full links needed to proceed to the page where xml is stored. \n",
    "        link_list = []\n",
    "        for x in document_id:\n",
    "            if x.has_attr('href'):\n",
    "                orange = x['href']\n",
    "                link_list.append(\"https://www.sec.gov\" + orange)\n",
    "\n",
    "        #run each link in 'link_list[]' through urllib, then beautifulSoup, and extract \n",
    "        #all data from the table using 'tr'. This will provide links for the form4 XML document\n",
    "        #store each iteration in 'xml_list'\n",
    "        xml_list = []\n",
    "        for link in link_list:\n",
    "            time.sleep(.5)\n",
    "            xml_data = urllib.request.urlopen(link).read()\n",
    "            form4_bs = bs.BeautifulSoup(xml_data)\n",
    "            form4_condensed = form4_bs.find_all('tr')       \n",
    "            xml_list.extend(form4_condensed)\n",
    "        \n",
    "        #extract only the link containing the XML document from each iteration of new_xml_list\n",
    "        #use 'text= re.compile(r\"\\.xml$\"' to find link anchor text that ends in .xml\n",
    "        #each .xml link will be an index in href_list[]\n",
    "        href_list = []\n",
    "        for x in xml_list:\n",
    "            xml_link = x.findAll('a', href=True, text=re.compile(r\"form13fInfoTable.xml$\"))    \n",
    "            href_list.extend(xml_link)\n",
    "\n",
    "\n",
    "        #remove tags and append \"https://www.sec.gov\" to links in href_list[] and \n",
    "        #store in new list new_href_list[]\n",
    "        new_href_list = []\n",
    "        for x in href_list:\n",
    "            if x.has_attr('href'):\n",
    "                purple = x['href']\n",
    "                new_href_list.append(\"https://www.sec.gov\" + purple) \n",
    "\n",
    "        #create a dictionary with each key being the column name for the values\n",
    "        diction_13f = {'nameOfIssuer': ([]),\n",
    "                        'titleOfClass': ([]),\n",
    "                        'cusip': ([]),\n",
    "                        'value': ([]),\n",
    "                        'sshPrnamt': ([]),\n",
    "                        'sshPrnamtType': ([]),\n",
    "                        'investmentDiscretion': ([]),\n",
    "                        'otherManager': ([]),\n",
    "                        'Sole': ([]),\n",
    "                        'Shared': ([]),\n",
    "                        'None': ([])}\n",
    "        \n",
    "        #create a dataframe for every .xml document link stored in new_href_list[]\n",
    "        x_xml_links = []\n",
    "        for link in new_href_list:\n",
    "            print(link)\n",
    "            time.sleep(.5)\n",
    "            x_data = urllib.request.urlopen(link).read()\n",
    "            source_bs = bs.BeautifulSoup(x_data, 'xml')\n",
    "        \n",
    "            #Each index of list_info will contain tags and text between infotable opening and closing tag\n",
    "            list_info = []\n",
    "            for a in source_bs.find_all('infoTable'):\n",
    "                list_info.append(a)\n",
    "              \n",
    "\n",
    "            #iterate through beautifulSoup content in list_info, searching for tags with key value names found in diction_13f, \n",
    "            #append this information to diction_13f\n",
    "            for x in list_info:\n",
    "                for key in diction_13f:\n",
    "                    diction_append = x.find(str(key))\n",
    "                    if diction_append:\n",
    "                        word_text = diction_append.text\n",
    "                    else:\n",
    "                        word_text = 'N/A'\n",
    "                    diction_13f[key].append(word_text)\n",
    "                    \n",
    "        \n",
    "        #convert diction_13f to a dataframe. If the dataframe is empty then pass, else output to json file. \n",
    "        df = pd.DataFrame(diction_13f)\n",
    "        if df.empty:\n",
    "            pass\n",
    "        else:\n",
    "            print(df)\n",
    "            #output dictionary to json\n",
    "            with open('/Users/a/Desktop/KSU/GRA/CSV/13f/13f.json', 'a') as json_text:\n",
    "                json.dump(diction_13f, json_text)\n",
    "            print(\"output complete\")\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
